# 2025Tpami
2025Tpami：代码开源，整体思路解析

## 工作过程问题总结（逐步整理更新）
（1）：基础视觉语言大模型对Office-Caltech跨域数据集的强大影响

动机：实验过程中，通过分配客户端组给Office-Caltech实现数据异质，对比2024cvpr&2023cvpr，3214客户端组划分策略，每个客户端抽取该域数据集0.2的数据量，引入clip作为backbond（对比resnet10）fedavg聚合效果98%，采取更极端划分，只分配0.01数据量，发现结果还是很高，修改客户端组为1111，数据量0.01，各个数据域划分数据为1，训练效果仍然很高

推测原因：

  1:训练集和测试集图像之间极度相似，即使在少样本情况下，仍可以使训练效果极好（clip特征相似度高） 
  
  2:类间差异性大，即使clip下文本图像相似度不太高，图片与图片之间相似度不高，但是十个类之间很好区分。
  
后面可以补充一个小实验，对Office-Caltech的四个数据集分别进行clip测试，具体实验：拿一个数据集举例，首先是十个类各取一张图片，作为训练图片参照，然后拿一张训练图片参照与十个类各随机抽的一张图片，一共十张图片做相似度计算，可以证明即使样本只有一个，分类效果也很好

（2）：clip backbond下分类任务数量的探究

动机：在工作中我们发现在clip作为backbond的情况下，对多个跨域数据集（digits、Office-Caltech、pacs）进行特征提取，参照2024cvpr&2023cvpr&2024Tpami的设置，联邦（Fedavg）架构下精度高达97%，我们进行了更极端的训练划分，多round收敛后仍然能达到95%，最极端情况下，我们设置各个域只被一个客户端划分，各个客户端的各个类只划分一个样本，联邦聚合后精度仍有90%
基于此现象，我们希望对当前多个主流联邦架构，进行clip+联邦架构的性能分析，分析传统设置，少样本设置，零样本设置分析

推测原因：

 1:诸如clip、dino v2这样的视觉语言基础大模型，在面临多分类任务场景，表现出60-70较低的精度，在对于十分类cifar-10、少分类跨域数据集而言，表现出较高的精度，得益于clip图像解码器的相似度分析，由于训练clip的策略是对比学习，开放学习，持续学习，导致clip对于万物都有一定的识别，直观的就是对于Office-Caltech的随便一张bike图片，使用文本图片相似度、图片余弦相似度分析，都能取得不低的效果，这种效果在少分类问题，尤其是类间差异大的情况下将会被进一步放大
 
 2:跨域数据集中虽然提出了跨域，但是不同域的相同类仍然具有较大的相似性和一致性，特别的就是Office-Caltech的样本

 据此我们希望在实验后，提出少分类跨域问题将被诸如clip这样的大模型解决，我们将进一步研究多分类跨域问题



## 文章注意点：

1：对比tpami2024的工作，在狄利克雷较高的时候，我们已经达到了很高的水平


## 笔记存储

1：在 MOON 中，虽然使用了对比学习损失和近端正则项来约束本地模型训练，但是在参数聚合阶段，MOON 依然使用了与 FedAvg 类似的聚合策略来进行全局模型的更新。因此，MOON 的联邦学习架构在参数聚合部分与 FedAvg 是类似的，即仍然使用“全局模型等于所有客户端模型的平均值”来更新全局模型的权重。
MOON 的主要区别在于它在客户端模型的本地训练过程中增加了对比学习损失和近端项约束，而不是在聚合策略上与 FedAvg 完全不同。MOON 仍然使用 FedAvg 样式的权重聚合，因为这是标准的联邦学习参数聚合方法。

2：在 FedProx 中，虽然客户端更新时加入了近端项，但全局模型的参数聚合方式仍然可以使用 FedAvg 聚合策略。这是因为 FedProx 本质上是在客户端的本地训练过程中加入了对全局模型的约束，即引入了近端项，以减少本地模型和全局模型的偏差。然而，在全局模型更新时，仍然可以使用 FedAvg 进行参数聚合。

3：FedProto 是一种联邦学习的变种，通过在联邦学习过程中引入原型学习，使每个客户端可以使用全局共享的类原型来进行训练。它与其他联邦方法不同，联邦聚合是通过计算客户端本地数据的类原型并与全局原型对比，来调整模型的训练。
每个客户端将计算并更新本地类原型。全局模型聚合所有客户端的类原型。训练过程中通过类原型来进行对比学习。

分类损失 (classification_loss)：模型输出和真实标签的交叉熵损失。
原型对比损失 (proto_loss)：本地模型的权重与类原型之间的差异，用于进行原型对比学习。
近端项 (proximal_term)：本地模型与全局模型的参数差异，使用 λ 控制其权重，类似 FedProx 中的近端正则化。

4：FedOpt 是基于 FedAvg 的一种改进方法，它引入了服务器端的优化器，而不是单纯的在服务器端进行模型权重的平均。FedOpt 的目的是使用优化器在服务器端更新全局模型，增强对异质数据的适应性。
通过服务器端优化器的引入，FedOpt 可以加快模型的收敛速度，尤其是在异质数据集上。
允许在服务器端进行更灵活的全局优化，而不仅仅依赖客户端本地的更新。

5：FedNTD 是一种基于知识蒸馏的联邦学习方法，旨在应对不同客户端之间数据分布差异较大的问题。FedNTD 通过知识蒸馏的方式，使每个客户端能够从其他客户端学习，从而提升全局模型的泛化能力。



